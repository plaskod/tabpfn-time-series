{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62aa347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./mitra_regressor_model\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #79-Ubuntu SMP Fri Apr 25 14:51:39 UTC 2025\n",
      "CPU Count:          128\n",
      "Memory Avail:       487.19 GB / 503.53 GB (96.8%)\n",
      "Disk Space Avail:   315.94 GB / 1758.86 GB (18.0%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/dplaskow/pfn-gift-eval/tabpfn-time-series/gift_eval/plotting/mitra_regressor_model\"\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 8\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    498880.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'MITRA': [{'fine_tune': False}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: Mitra ...\n",
      "\tFitting with cpus=64, gpus=1, mem=7.1/487.2 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "California Housing: (20640, 9)\n",
      "Training set sizes:\n",
      "Housing: 16512 samples\n",
      "Training Mitra regressor on California Housing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5628\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'Mitra': 1.0}\n",
      "\t-0.5628\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.59s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5571.1 rows/s (200 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/dplaskow/pfn-gift-eval/tabpfn-time-series/gift_eval/plotting/mitra_regressor_model\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitra</td>\n",
       "      <td>-0.545477</td>\n",
       "      <td>-0.562759</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.333246</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.804209</td>\n",
       "      <td>0.333246</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.804209</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.545477</td>\n",
       "      <td>-0.562759</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.335339</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val              eval_metric  \\\n",
       "0                Mitra   -0.545477  -0.562759  root_mean_squared_error   \n",
       "1  WeightedEnsemble_L2   -0.545477  -0.562759  root_mean_squared_error   \n",
       "\n",
       "   pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  \\\n",
       "0        0.333246       0.035654  0.804209                 0.333246   \n",
       "1        0.335339       0.035900  0.805833                 0.002093   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.035654           0.804209            1       True   \n",
       "1                0.000246           0.001624            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load datasets\n",
    "housing_data = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "housing_df['target'] = housing_data.target\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"California Housing: {housing_df.shape}\")\n",
    "\n",
    "# Create train/test splits (80/20)\n",
    "housing_train, housing_test = train_test_split(housing_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set sizes:\")\n",
    "print(f\"Housing: {len(housing_train)} samples\")\n",
    "\n",
    "# Convert to TabularDataset\n",
    "housing_train_data = TabularDataset(housing_train)\n",
    "housing_test_data = TabularDataset(housing_test)\n",
    "\n",
    "# Create predictor with Mitra for regression\n",
    "print(\"Training Mitra regressor on California Housing dataset...\")\n",
    "mitra_reg_predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='./mitra_regressor_model',\n",
    "    problem_type='regression'\n",
    ")\n",
    "mitra_reg_predictor.fit(\n",
    "    housing_train_data.sample(1000), # sample 1000 rows\n",
    "    hyperparameters={\n",
    "        'MITRA': {'fine_tune': False}\n",
    "    },\n",
    ")\n",
    "\n",
    "# Evaluate regression performance\n",
    "mitra_reg_predictor.leaderboard(housing_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045de404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "\n",
    "mm = mitra_reg_predictor._trainer.load_model('Mitra')        # or 'TabPFNv2'\n",
    "pt = mm.model.trainers[0].model               # the underlying torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64eb59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tab2D(\n",
       "  (x_quantile): Tab2DQuantileEmbeddingX()\n",
       "  (x_embedding): Tab2DEmbeddingX(\n",
       "    (x_embedding): Linear(in_features=1, out_features=512, bias=True)\n",
       "  )\n",
       "  (y_embedding): Tab2DEmbeddingYRegression(\n",
       "    (y_embedding): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (y_mask): Embedding(1, 512)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Layer(\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention1): MultiheadAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention2): MultiheadAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (linear4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
